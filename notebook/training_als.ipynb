{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download(\"PATH/TO/FILE\")\n",
    "\n",
    "# tar xvf {file.zip}\n",
    "# tar -xvf {file.zip}\n",
    "# tar xvf {file.zip} -C /dest/directory/\n",
    "\n",
    "# using python\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile(\"file.zip\",\"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"targetdir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_matrix(df, with_index=False):\n",
    "    matrix = df.pivot_table(columns=['item_id'], index=['user_id'], values='rating')\n",
    "    if with_index:\n",
    "        return matrix.fillna(0).as_matrix(), matrix.index, matrix.columns\n",
    "    else:\n",
    "        return matrix.fillna(0).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairnessRegALS:\n",
    "    def __init__(self, df_train, n_factor, user_factor=None, item_factor=None):\n",
    "\n",
    "        # get data from dataframe\n",
    "        self.data_frame = df_train\n",
    "        R_matrix, user_index, item_index = dataframe_to_matrix(df_train, with_index=True)\n",
    "        self.R = R_matrix\n",
    "        self.user_index = user_index\n",
    "        self.item_index = item_index\n",
    "\n",
    "        self.R_prediction = None\n",
    "        self.n_user, self.n_item = self.R.shape\n",
    "\n",
    "        # common parameter\n",
    "        self.n_factor = n_factor\n",
    "\n",
    "        # rank als parameter\n",
    "        self.is_support_weight = False\n",
    "\n",
    "        # initialize user (P) & item (Q) latent model\n",
    "        if user_factor is None and item_factor is None:\n",
    "            self.P = np.random.rand(self.n_user, self.n_factor)\n",
    "            self.Q = np.random.rand(self.n_item, self.n_factor)\n",
    "        else:\n",
    "            self.P = user_factor\n",
    "            self.Q = item_factor\n",
    "\n",
    "        # DEBUGGING PURPOSE ONLY, CONSTANT FACTOR CAN RAISING ERROR!.\n",
    "        # self.constant_latent_factor()\n",
    "\n",
    "        # check matrix shape dimension\n",
    "        if self.P.shape != (self.n_user, self.n_factor) \\\n",
    "                or self.Q.shape != (self.n_item, self.n_factor):\n",
    "            raise ValueError\n",
    "\n",
    "        # building support weight\n",
    "        self.support_weight_vector = np.zeros(self.n_item)\n",
    "        self.support_weight_sum = 0\n",
    "        for item_idx in range(self.n_item):\n",
    "            support_value = np.count_nonzero(self.R[:, item_idx]) if self.is_support_weight else 1\n",
    "            self.support_weight_vector[item_idx] = support_value\n",
    "            self.support_weight_sum += support_value\n",
    "\n",
    "    def train_data(self, iteration, directory=None):\n",
    "        for iterate in range(iteration):\n",
    "            print(\"als train_data iteration {} at {}\".format(iterate + 1, datetime.now()))\n",
    "            # P STEP: UPDATE USER VECTORS\n",
    "            # q̃ = QTs,\n",
    "            # Ã = QT diag(s)Q\n",
    "            q_tilde = np.zeros(self.n_factor)\n",
    "            A_tilde = np.zeros((self.n_factor, self.n_factor))\n",
    "            for item in range(self.n_item):\n",
    "                qj = self.Q[item, :]\n",
    "                sj = self.support_weight_vector[item]\n",
    "                q_tilde += qj * sj\n",
    "                A_tilde = A_tilde + np.outer(qj, qj)\n",
    "\n",
    "            # for u ← 1,..., U do ;\n",
    "            # TODO: RE-CHECK THIS, IT HAS SIDE-EFFECT IN DATA-SET (R)\n",
    "            cus = [i for i in range(self.n_user) if np.any(self.R[i, :])]\n",
    "\n",
    "            for user in cus:\n",
    "                A_bar = np.zeros((self.n_factor, self.n_factor))\n",
    "                q_bar = np.zeros(self.n_factor)\n",
    "                b_bar = np.zeros(self.n_factor)\n",
    "                b_tilde = np.zeros(self.n_factor)\n",
    "\n",
    "                Ru = self.filter_row(self.R[user])  # OK\n",
    "                I_bar = len(Ru)  # OK\n",
    "                r_tilde, r_bar = 0.0, 0.0  # OK\n",
    "\n",
    "                for i, rui in Ru:\n",
    "                    qi = self.Q[i]  # OK\n",
    "\n",
    "                    A_bar = A_bar + np.outer(qi, qi)\n",
    "\n",
    "                    q_bar = q_bar + qi\n",
    "                    b_bar = b_bar + (qi * rui)\n",
    "                    si = self.support_weight_vector[i]\n",
    "                    r_tilde += si * rui\n",
    "                    r_bar += rui\n",
    "                    b_tilde = b_tilde + qi * (si * rui)\n",
    "\n",
    "                M = A_bar * self.support_weight_sum \\\n",
    "                    - (np.outer(q_bar, q_tilde)) \\\n",
    "                    - (np.outer(q_tilde, q_bar)) \\\n",
    "                    + (A_tilde * I_bar)\n",
    "\n",
    "                y = b_bar * self.support_weight_sum \\\n",
    "                    - (q_bar * r_tilde) \\\n",
    "                    - (q_tilde * r_bar) \\\n",
    "                    + (b_tilde * I_bar)\n",
    "\n",
    "                pu = np.linalg.inv(M).dot(y)\n",
    "                self.P[user, :] = pu\n",
    "\n",
    "            ###################################################\n",
    "            #  CHECKPOINT - MANUALLY CHECKED                  #\n",
    "            ###################################################\n",
    "            # Q STEP: UPDATE ITEM VECTORS\n",
    "\n",
    "            # k, v -> int, double\n",
    "            map_p1_tensor = {}\n",
    "            map_p3_tensor = {}\n",
    "            map_b_tensor = {}\n",
    "\n",
    "            # k, v -> int, []\n",
    "            map_p2_tensor = {}\n",
    "\n",
    "            # for each user\n",
    "            for user in cus:\n",
    "                Ru = self.filter_row(self.R[user])  # this is okay\n",
    "\n",
    "                sum_p1_tensor = 0.0\n",
    "                sum_p3_tensor = 0.0\n",
    "                sum_b_tensor = len(Ru)\n",
    "                sum_p2_tensor = np.zeros(self.n_factor)\n",
    "\n",
    "                for j, ruj in Ru:\n",
    "                    sj = self.support_weight_vector[j]\n",
    "                    sum_p1_tensor += sj * ruj\n",
    "                    sum_p3_tensor += ruj\n",
    "\n",
    "                    sum_p2_tensor = sum_p2_tensor + self.Q[j]\n",
    "\n",
    "                map_p1_tensor[user] = sum_p1_tensor\n",
    "                map_p3_tensor[user] = sum_p3_tensor\n",
    "                map_b_tensor[user] = sum_b_tensor\n",
    "                map_p2_tensor[user] = sum_p2_tensor\n",
    "\n",
    "            # FAIRNESS REGULARIZATION\n",
    "            # dist = lambda param1, param2: param1 + param2\n",
    "            # sum_d_q = np.zeros((self.n_item, self.n_factor))\n",
    "            # for item_i in range(self.n_item):\n",
    "            #     for item_j in range(self.n_item):\n",
    "            #         # update sum_d_q\n",
    "            #         for x, y in np.nditer([sum_d_q[item_i], self.Q[item_j]], op_flags=['readwrite']):\n",
    "            #             x[...] = dist(x, y)\n",
    "\n",
    "            # for each item\n",
    "            for item in range(self.n_item):\n",
    "                A_bar = np.zeros((self.n_factor, self.n_factor))\n",
    "                A_tensor = np.zeros((self.n_factor, self.n_factor))\n",
    "                b_bar = np.zeros(self.n_factor)\n",
    "\n",
    "                p1_tensor = np.zeros(self.n_factor)\n",
    "                p3_tensor = np.zeros(self.n_factor)\n",
    "                b_tensor = np.zeros(self.n_factor)\n",
    "                p2_tensor = np.zeros(self.n_factor)\n",
    "\n",
    "                si = self.support_weight_vector[item]\n",
    "                for user in cus:\n",
    "                    pu = self.P[user]\n",
    "                    rui = self.R[user, item]\n",
    "\n",
    "                    pp = np.outer(pu, pu)  # 6x6 indeed\n",
    "                    A_bar += pp\n",
    "\n",
    "                    p2_tensor = p2_tensor + pp.dot(map_p2_tensor.get(user))\n",
    "                    A_tensor = A_tensor + (pp * map_b_tensor.get(user))\n",
    "                    p3_tensor = p3_tensor + pu * map_p3_tensor.get(user)\n",
    "\n",
    "                    if rui > 0:\n",
    "                        b_bar += pu * rui\n",
    "                        p1_tensor += pu * map_p1_tensor.get(user)\n",
    "                        b_tensor += pu * (rui * map_b_tensor.get(user))\n",
    "\n",
    "                M = (A_bar * self.support_weight_sum) + (A_tensor * si)  # THIS IS DOPE\n",
    "                y = A_bar.dot(q_tilde) \\\n",
    "                    + (b_bar * self.support_weight_sum) \\\n",
    "                    - p1_tensor \\\n",
    "                    + (p2_tensor * si) \\\n",
    "                    - (p3_tensor * si) \\\n",
    "                    + (b_tensor * si)\n",
    "\n",
    "                # dope variable checked\n",
    "                # b_bar, p1_tensor, p2_tensor, p3_tensor, b_tensor\n",
    "\n",
    "                qi = np.linalg.inv(M).dot(y)\n",
    "                self.Q[item, :] = qi\n",
    "\n",
    "            if directory is not None:\n",
    "                print(\"model saved to {} at {}\".format(directory, datetime.now()))\n",
    "                self.save_data(directory)\n",
    "\n",
    "        # build matrix prediction after training\n",
    "        self.R_prediction = self.P.dot(self.Q.T)\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        :return: int number of rating predicted value\n",
    "        \"\"\"\n",
    "        user_idx = self.user_index.get_loc(user_id)\n",
    "        item_idx = self.item_index.get_loc(item_id)\n",
    "        return self.P[user_idx, :].dot(self.Q[item_idx, :].T)\n",
    "\n",
    "\n",
    "    def matrix_prediction(self):\n",
    "        if self.R_prediction is None:\n",
    "            self.R_prediction = self.P.dot(self.Q.T)\n",
    "        return self.R_prediction\n",
    "\n",
    "    def top_n_recommendation(self, user_id, n,\n",
    "                             return_index=False,\n",
    "                             with_index=False,\n",
    "                             with_reviewed=True):\n",
    "        \"\"\"\n",
    "        :return: slice of n item_idx\n",
    "        \"\"\"\n",
    "        user_idx = self.user_index.get_loc(user_id)\n",
    "\n",
    "        if self.R_prediction is None:\n",
    "            self.matrix_prediction()\n",
    "\n",
    "        if not with_reviewed:\n",
    "            non_watched_item = self.R_prediction[user_idx]\n",
    "        else:\n",
    "            watched = self.R[user_idx]\n",
    "            non_watched_index = np.where(watched == 0)\n",
    "            non_watched_item = self.R_prediction[user_idx][non_watched_index]\n",
    "\n",
    "        index_sorted = np.argsort(non_watched_item)[-n:]\n",
    "\n",
    "        # reversed because argsort cannot desc\n",
    "        rec_item_idx = np.array(list(reversed(index_sorted)))\n",
    "\n",
    "        # TODO: return_index option not awesome, will deprecated this\n",
    "        if return_index:\n",
    "            return rec_item_idx\n",
    "        elif with_index:\n",
    "            return [self.item_index[item] for item in rec_item_idx], rec_item_idx\n",
    "        else:\n",
    "            # convert list recommendation to id\n",
    "            return [self.item_index[item] for item in rec_item_idx]\n",
    "\n",
    "    def save_data(self, directory):\n",
    "        np.save(directory + \"/P.npy\", self.P)\n",
    "        np.save(directory + \"/Q.npy\", self.Q)\n",
    "        self.data_frame.to_pickle(directory + '/data_frame.pkl')\n",
    "        np.save(directory + \"/n_factor.npy\", self.n_factor)\n",
    "\n",
    "    # noinspection PyBroadException\n",
    "    @staticmethod\n",
    "    def load_data(directory):\n",
    "        try:\n",
    "            P = np.load(directory + \"/P.npy\")\n",
    "            Q = np.load(directory + \"/Q.npy\")\n",
    "            # R = np.load(directory + \"/R.npy\")\n",
    "            data_frame = pd.read_pickle(directory + '/data_frame.pkl')\n",
    "            n_factor = np.load(directory + \"/n_factor.npy\")\n",
    "\n",
    "            return FairnessRegALS(data_frame, n_factor, P, Q)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_row(vector):\n",
    "        return [(i, j) for i, j in enumerate(vector) if j != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from config import MODEL_LOCATION, DATASET_DIR\n",
    "from recommender.fairness_reg_als import FairnessRegALS\n",
    "from recommender.util import load_dataset, dataframe_to_matrix\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # load from previous model\n",
    "    als = FairnessRegALS.load_data('')\n",
    "\n",
    "    # create new if isn't available\n",
    "    if als is None:\n",
    "\n",
    "        # prepare dataset\n",
    "        ratings_df = load_dataset(DATASET_DIR)\n",
    "        train, test = train_test_split(ratings_df, test_size=0.2)\n",
    "        print(\"total user dataset: {}, item dataset: {}\"\n",
    "              .format(ratings_df.user_id.unique().shape, ratings_df.item_id.unique().shape))\n",
    "        print(\"total user training: {}, item training: {}\"\n",
    "              .format(train.user_id.unique().shape, train.item_id.unique().shape))\n",
    "        print(\"total user test: {}, item test: {}\"\n",
    "              .format(test.user_id.unique().shape, test.item_id.unique().shape))\n",
    "        print(\"total record training: {}, total record test: {}\"\n",
    "              .format(train.shape[0], test.shape[0]))\n",
    "\n",
    "        # create new recommender instance\n",
    "        als = FairnessRegALS(df_train=train, n_factor=50)\n",
    "\n",
    "    # train the recommender\n",
    "    als.train_data(iteration=30, directory=MODEL_LOCATION)\n",
    "    als.save_data(MODEL_LOCATION)\n",
    "\n",
    "# run the main\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('example.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
